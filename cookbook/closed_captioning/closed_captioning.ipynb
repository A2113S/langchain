{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Captioning\n",
    "Implements a feature using Langchain's image_captions.py and audio_speech_to_text.py to produce .srt files. This system will provide both subtitles and visual scene descriptions, essentially creating closed captioning.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for closed captioning\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import transformers\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from langchain.document_loaders import AssemblyAIAudioTranscriptLoader\n",
    "from langchain.document_loaders import ImageCaptionLoader\n",
    "from langchain.document_loaders.assemblyai import TranscriptFormat\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel:\n",
    "    def __init__(self, start_time, end_time, closed_caption):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.closed_caption = closed_caption\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"start_time: {self.start_time}, end_time: {self.end_time}, closed_caption: {self.closed_caption}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoModel:\n",
    "    def __init__(self, start_time, end_time, image_description):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.image_description = image_description\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"start_time: {self.start_time}, end_time: {self.end_time}, image_description: {self.image_description}\"\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self.start_time\n",
    "    \n",
    "    def get_end_time(self):\n",
    "        return self.end_time\n",
    "    \n",
    "    def get_image_description(self):\n",
    "        return self.image_description\n",
    "    \n",
    "    def set_start_time(self, start_time):\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def set_end_time(self, end_time):\n",
    "        self.end_time = end_time\n",
    "    \n",
    "    def set_image_description(self, image_description):\n",
    "        self.image_description = image_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioModel:\n",
    "    def __init__(self, start_time, end_time, subtitle_text):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.subtitle_text = subtitle_text\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"start_time: {self.start_time}, end_time: {self.end_time}, subtitle_text: {self.subtitle_text}\"\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self.start_time\n",
    "    \n",
    "    def get_end_time(self):\n",
    "        return self.end_time\n",
    "    \n",
    "    def get_subtitle_text(self):\n",
    "        return self.subtitle_text\n",
    "    \n",
    "    def set_start_time(self, start_time):\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def set_end_time(self, end_time):\n",
    "        self.end_time = end_time\n",
    "    \n",
    "    def set_subtitle_text(self, subtitle_text):\n",
    "        self.subtitle_text = subtitle_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"test_data/test.mp3\"\n",
    "\n",
    "loader = AssemblyAIAudioTranscriptLoader(\n",
    "    file_path=audio_file, \n",
    "    api_key=\"f50c08e20ecd4544b175953636f0b936\", \n",
    "    transcript_format=TranscriptFormat.SUBTITLES_SRT\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "def CreateTranscriptModel(doc):\n",
    "    transcription = doc.strip().split(\"\\n\")\n",
    "    times = transcription[1].split(\" --> \")\n",
    "    start_time = times[0].strip()\n",
    "    end_time = times[1].strip()\n",
    "\n",
    "    subtitle_text = ' '.join(transcription[2:]).strip()\n",
    "\n",
    "    transcript_model = AudioModel(start_time, end_time, subtitle_text)\n",
    "\n",
    "    return transcript_model\n",
    "\n",
    "print(CreateTranscriptModel(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Split Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mp3(mp4_path):\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(mp4_path)\n",
    "        .output(\"test_data/audio.mp3\", format=\"mp3\")\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "def CreateTranscriptModels(doc):\n",
    "    subtitles = doc.strip().split(\"\\n\\n\")  # Splitting based on double newline, which separates SRT entries\n",
    "    models = []\n",
    "\n",
    "    for subtitle in subtitles:\n",
    "        lines = subtitle.split(\"\\n\")\n",
    "        if len(lines) >= 3:  # Checking if there are enough lines for an index, timestamp, and text\n",
    "            times = lines[1].split(\" --> \")\n",
    "            start_time = times[0].strip()\n",
    "            end_time = times[1].strip()\n",
    "\n",
    "            subtitle_text = ' '.join(lines[2:]).strip()\n",
    "\n",
    "            transcript_model = AudioModel(start_time, end_time, subtitle_text)\n",
    "            models.append(transcript_model)\n",
    "\n",
    "    return models\n",
    "\n",
    "loader = AssemblyAIAudioTranscriptLoader(\n",
    "    file_path=\"test_data/audio.mp3\", \n",
    "    api_key=\"f50c08e20ecd4544b175953636f0b936\", \n",
    "    transcript_format=TranscriptFormat.SUBTITLES_SRT\n",
    ")\n",
    "\n",
    "# Assuming loader.load() returns the full transcript in a single document\n",
    "docs = loader.load()\n",
    "\n",
    "# This will now create a list of lists of AudioModel instances\n",
    "all_audio_models = [CreateTranscriptModels(doc.page_content) for doc in docs]\n",
    "\n",
    "# Flatten the list if necessary\n",
    "audio_models = [model for sublist in all_audio_models for model in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 00:00:00,570, end_time: 00:00:03,966, subtitle_text: Let's make tea. Duet with me. You read red,\n",
      "start_time: 00:00:04,068, end_time: 00:00:06,910, subtitle_text: I read green. Can I get you a drink?\n",
      "start_time: 00:00:09,250, end_time: 00:00:10,750, subtitle_text: Tea or coffee?\n",
      "start_time: 00:00:12,370, end_time: 00:00:13,920, subtitle_text: How do you take it?\n",
      "start_time: 00:00:16,050, end_time: 00:00:16,800, subtitle_text: Here.\n",
      "start_time: 00:00:19,810, end_time: 00:00:22,400, subtitle_text: Great work. Now let's try it again.\n"
     ]
    }
   ],
   "source": [
    "for audio_model in audio_models:\n",
    "    print(audio_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Split to Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_difference(prev_frame, curr_frame, threshold=30):\n",
    "    # Compute the absolute difference between the current frame and the previous frame\n",
    "    diff = cv2.absdiff(prev_frame, curr_frame)\n",
    "    # Thresholding to get the binary image, where white represents significant difference\n",
    "    _, thresh = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    # If there are any white pixels in thresh, the difference is significant\n",
    "    return np.any(thresh)\n",
    "\n",
    "# Initialize the video capture\n",
    "capture = cv2.VideoCapture('test_data/eng_convo.mp4')\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "frame_duration = 1000 / fps\n",
    "\n",
    "video_models = []\n",
    "\n",
    "frameNr = 0\n",
    "ret, prev_frame = capture.read()\n",
    "prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if ret else None\n",
    "prev_start_time = 0\n",
    "start_time = 0\n",
    "\n",
    "while ret:\n",
    "    end_time = prev_start_time\n",
    "    prev_start_time = capture.get(cv2.CAP_PROP_POS_MSEC)\n",
    "\n",
    "    ret, frame = capture.read()\n",
    "    if not ret:\n",
    "        start_time = end_time + frame_duration\n",
    "        break\n",
    "    \n",
    "    start_time = prev_start_time\n",
    "    # Convert to grayscale for comparison\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compare with the previous frame\n",
    "    if frameNr == 0 or frame_difference(prev_frame_gray, frame_gray):\n",
    "        end_time = capture.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        cv2.imwrite(f'test_data/output_frames/frame.jpg', frame)\n",
    "        prev_frame_gray = frame_gray\n",
    "\n",
    "        # Define the path to the \"output_frames\" folder\n",
    "        folder_path = f'test_data/output_frames/'\n",
    "\n",
    "        # List all .jpg files in the folder\n",
    "        image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".jpg\")]\n",
    "\n",
    "        # Create an instance of the ImageCaptionLoader\n",
    "        loader = ImageCaptionLoader(images=image_files)\n",
    "\n",
    "        # Load captions for the images\n",
    "        list_docs = loader.load()\n",
    "\n",
    "        video_model = VideoModel(start_time, end_time, list_docs[len(list_docs) - 1].page_content.replace(\"[SEP]\", \"\").strip())\n",
    "        video_models.append(video_model)\n",
    "\n",
    "        frameNr += 1\n",
    "\n",
    "# Release the video capture object\n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 0.0, end_time: 40.0, image_description: an image of a kitchen area in a building\n",
      "start_time: 480.0, end_time: 2240.0, image_description: a young girl in various settings\n",
      "start_time: 2320.0, end_time: 9320.0, image_description: footage of a woman in a sweater appears repeatedly throughout this segment\n",
      "start_time: 12360.0, end_time: 14200.0, image_description: the same image of a woman in a sweater is shown at multiple intervals\n",
      "start_time: 15960.0, end_time: 22360.0, image_description: multiple instances of a woman holding a cup of coffee\n"
     ]
    }
   ],
   "source": [
    "for video_model in video_models:\n",
    "    print(video_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio and Video Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 0.0, end_time: 40.0, image_description: an image of a kitchen area in a building\n",
      "start_time: 480.0, end_time: 2240.0, image_description: a young girl in various settings\n",
      "start_time: 2320.0, end_time: 9320.0, image_description: footage of a woman in a sweater appears repeatedly throughout this segment\n",
      "start_time: 12360.0, end_time: 14200.0, image_description: the same image of a woman in a sweater is shown at multiple intervals\n",
      "start_time: 15960.0, end_time: 22360.0, image_description: multiple instances of a woman holding a cup of coffee\n"
     ]
    }
   ],
   "source": [
    "# Assuming the VideoModel class definition is already in scope\n",
    "\n",
    "caption_data = \"\"\"\n",
    "start_time: 0.0, end_time: 40.0, image_description: an image of a kitchen area in a building\n",
    "start_time: 480.0, end_time: 2240.0, image_description: a young girl in various settings, last seen wearing a gray sweater\n",
    "start_time: 2320.0, end_time: 9320.0, image_description: footage of a woman in a sweater appears repeatedly throughout this segment\n",
    "start_time: 12360.0, end_time: 14200.0, image_description: the same image of a woman in a sweater is shown at multiple intervals\n",
    "start_time: 15960.0, end_time: 22360.0, image_description: multiple instances of a woman holding a cup of coffee\n",
    "\"\"\"\n",
    "\n",
    "# Parsing the raw caption data and instantiating VideoModel objects\n",
    "video_models = []\n",
    "for line in caption_data.strip().split(\"\\n\"):\n",
    "    if not line.strip():\n",
    "        continue  # Skip empty lines\n",
    "    parts = line.split(\",\")\n",
    "    start_time = float(parts[0].split(\":\")[1].strip())\n",
    "    end_time = float(parts[1].split(\":\")[1].strip())\n",
    "    image_description = parts[2].split(\":\")[1].strip()\n",
    "\n",
    "    video_model = VideoModel(start_time, end_time, image_description)\n",
    "    video_models.append(video_model)\n",
    "\n",
    "# Display the created VideoModel objects\n",
    "for vm in video_models:\n",
    "    print(vm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Analyzing the coherence between subtitle and image description.\n",
      "Human: I will provide you with a subtitle from an audio track and a description of an image from the same moment in a video. Your task is to analyze whether the subtitle and the image description logically align and make sense together. If they do, just give back the image description provided. If they don't align well, creatively adjust the image description to better fit the subtitle while retaining the main idea of the image. Your response should be a suitable closed caption that combines both elements coherently if need be. Here are the details: Subtitle: Great work. Now let's try it again., Image Description: multiple instances of a woman holding a cup of coffee. Based on this, provide a suitable closed caption or adjust the image description to create one that makes sense with the subtitle. Ex. subtitle: great work, now lets try again and image: Woman holding a coffee cup. These two make sense because the woman could be talking so we would want to keep [Woman holding coffee cup] as our closed caption. So you would return the same image description. Another ex. subtitle: go fetch the ball and the image is: a stick flying. Then we can change the closed caption to: the ball is being thrown or something as our closed caption. So you return this new closed caption. IMPORTANT: the output should be 'Result:text' where text is the closed caption and keep the closed caption short and concise.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result: Woman practicing holding a coffee cup multiple times.\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChatOpenAI with the appropriate model and API key\n",
    "chat = ChatOpenAI(model=\"gpt-4\", max_tokens=4000, openai_api_key=\"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\")\n",
    "\n",
    "data = f\"I will provide you with a subtitle from an audio track and a description of an image from the same moment in a video. Your task is to analyze whether the subtitle and the image description logically align and make sense together. If they do, just give back the image description provided. If they don't align well, creatively adjust the image description to better fit the subtitle while retaining the main idea of the image. Your response should be a suitable closed caption that combines both elements coherently if need be. Here are the details: Subtitle: {audio_model.get_subtitle_text()}, Image Description: {video_model.get_image_description()}. Based on this, provide a suitable closed caption or adjust the image description to create one that makes sense with the subtitle. Ex. subtitle: great work, now lets try again and image: Woman holding a coffee cup. These two make sense because the woman could be talking so we would want to keep [Woman holding coffee cup] as our closed caption. So you would return the same image description. Another ex. subtitle: go fetch the ball and the image is: a stick flying. Then we can change the closed caption to: the ball is being thrown or something as our closed caption. So you return this new closed caption. IMPORTANT: the output should be 'Result:text' where text is the closed caption and keep the closed caption short and concise.\"\n",
    "\n",
    "# Construct the prompt for OpenAI using ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=\"Analyzing the coherence between subtitle and image description.\"\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{closed_caption}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Get response from OpenAI using LLMChain\n",
    "response = conversation({\"closed_caption\": data})\n",
    "\n",
    "print(response[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(s):\n",
    "    \"\"\"Converts a time string into seconds.\"\"\"\n",
    "    h, m, s = map(float, s.replace(',', '.').split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "def milliseconds_to_srt_time(ms):    \n",
    "    if isinstance(ms, str) and ',' in ms:        \n",
    "        return ms\n",
    "\n",
    "    \"\"\"Converts milliseconds to SRT time format 'HH:MM:SS,mmm'.\"\"\"\n",
    "    hours = int(ms // 3600000)\n",
    "    minutes = int((ms % 3600000) // 60000)\n",
    "    seconds = int((ms % 60000) // 1000)\n",
    "    milliseconds = int(ms % 1000)\n",
    "\n",
    "    return f'{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}'\n",
    "\n",
    "\n",
    "def find_overlapping_video_models(audio_model, video_models):\n",
    "    overlapping_models = []\n",
    "    audio_start = parse_time(str(audio_model.get_start_time()))\n",
    "    audio_end = parse_time(str(audio_model.get_end_time()))\n",
    "\n",
    "    for model in video_models:\n",
    "        video_start = model.get_start_time()  # Assuming already in milliseconds\n",
    "        video_end = model.get_end_time()  # Assuming already in milliseconds\n",
    "        if max(audio_start, video_start) < min(audio_end, video_end):\n",
    "            overlapping_models.append(model)\n",
    "\n",
    "\n",
    "    return overlapping_models\n",
    "\n",
    "\n",
    "def validate_and_adjust_description(audio_model, video_model):\n",
    "\n",
    "    # Initialize ChatOpenAI with the appropriate model and API key\n",
    "    chat = ChatOpenAI(model=\"gpt-4\", max_tokens=4000, openai_api_key=\"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\")\n",
    "\n",
    "    data = f\"I will provide you with a subtitle from an audio track and a description of an image from the same moment in a video. Your task is to analyze whether the subtitle and the image description logically align and make sense together. If they do, just give back the image description provided. If they don't align well, creatively adjust the image description to better fit the subtitle while retaining the main idea of the image. Your response should be a suitable closed caption that combines both elements coherently if need be. Here are the details: Subtitle: {audio_model.get_subtitle_text()}, Image Description: {video_model.get_image_description()}. Based on this, provide a suitable closed caption or adjust the image description to create one that makes sense with the subtitle. Ex. subtitle: great work, now lets try again and image: Woman holding a coffee cup. These two make sense because the woman could be talking so we would want to keep [Woman holding coffee cup] as our closed caption. So you would return the same image description. Another ex. subtitle: go fetch the ball and the image is: a stick flying. Then we can change the closed caption to: the ball is being thrown or something as our closed caption. So you return this new closed caption. IMPORTANT: the output should be 'Result:text' where text is the closed caption and keep the closed caption short and concise. Do not include the subtitle in the result.\"\n",
    "\n",
    "    # Construct the prompt for OpenAI using ChatPromptTemplate\n",
    "    prompt = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessage(\n",
    "                content=\"Analyzing the coherence between subtitle and image description.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"{closed_caption}\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    conversation = LLMChain(\n",
    "        llm=chat,\n",
    "        prompt=prompt,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Get response from OpenAI using LLMChain\n",
    "    response = conversation({\"closed_caption\": data})\n",
    "\n",
    "    return response[\"text\"]\n",
    "\n",
    "def create_caption_models(video_models, audio_models):\n",
    "    caption_models = []\n",
    "\n",
    "    # Iterate through each audio model\n",
    "    for audio_model in audio_models:\n",
    "        overlapping_video_models = find_overlapping_video_models(audio_model, video_models)\n",
    "\n",
    "        if overlapping_video_models:\n",
    "            # Create separate captions for each overlapping video model\n",
    "            for video_model in overlapping_video_models:\n",
    "                caption_text = f\"[{video_model.get_image_description()}] {audio_model.get_subtitle_text()}\"\n",
    "                caption_model = CaptionModel(milliseconds_to_srt_time(audio_model.get_start_time()), milliseconds_to_srt_time(audio_model.get_end_time()), caption_text)\n",
    "                caption_models.append(caption_model)\n",
    "        else:\n",
    "            # No overlapping video, use audio model's subtitle\n",
    "            caption_text = f\"[No video description] {audio_model.get_subtitle_text()}\"\n",
    "            caption_model = CaptionModel(milliseconds_to_srt_time(audio_model.get_start_time()), milliseconds_to_srt_time(audio_model.get_end_time()), caption_text)\n",
    "            caption_models.append(caption_model)\n",
    "\n",
    "    return caption_models\n",
    "\n",
    "\n",
    "\n",
    "# Create caption models\n",
    "caption_models = create_caption_models(video_models, audio_models)\n",
    "\n",
    "# Now, caption_models contains your finalized caption models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 00:00:00,000, end_time: 00:00:00,003, closed_caption: [an image of a kitchen area in a building] Let's make tea. Duet with me. You read red,\n",
      "start_time: 00:00:00,004, end_time: 00:00:00,006, closed_caption: [an image of a kitchen area in a building] I read green. Can I get you a drink?\n",
      "start_time: 00:00:00,009, end_time: 00:00:00,010, closed_caption: [an image of a kitchen area in a building] Tea or coffee?\n",
      "start_time: 00:00:00,012, end_time: 00:00:00,013, closed_caption: [an image of a kitchen area in a building] How do you take it?\n",
      "start_time: 00:00:00,016, end_time: 00:00:00,016, closed_caption: [an image of a kitchen area in a building] Here.\n",
      "start_time: 00:00:00,019, end_time: 00:00:00,022, closed_caption: [an image of a kitchen area in a building] Great work. Now let's try it again.\n"
     ]
    }
   ],
   "source": [
    "# print all caption models\n",
    "for caption_model in caption_models:\n",
    "    print(caption_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sRT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_srt_entry(index, caption_model):\n",
    "    \"\"\"Formats a single caption model into an SRT entry.\"\"\"\n",
    "    start_time = caption_model.start_time\n",
    "    end_time = caption_model.end_time\n",
    "    text = caption_model.closed_caption\n",
    "\n",
    "    return f\"{index}\\n{start_time} --> {end_time}\\n{text}\\n\"\n",
    "\n",
    "\n",
    "def generate_srt_content(caption_models):\n",
    "    \"\"\"Generates the full SRT content from a list of caption models.\"\"\"\n",
    "    srt_entries = []\n",
    "    for index, model in enumerate(caption_models, start=1):\n",
    "        srt_entry = format_srt_entry(index, model)\n",
    "        srt_entries.append(srt_entry)\n",
    "\n",
    "    return '\\n'.join(srt_entries)\n",
    "\n",
    "\n",
    "def write_srt_file(caption_models, file_name):\n",
    "    \"\"\"Writes the caption models to an SRT file.\"\"\"\n",
    "    srt_content = generate_srt_content(caption_models)\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(srt_content)\n",
    "\n",
    "# Usage\n",
    "write_srt_file(caption_models, 'test_data/output.srt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
