{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Captioning\n",
    "Implements a feature using Langchain's image_captions.py and audio_speech_to_text.py to produce .srt files. This system will provide both subtitles and visual scene descriptions, essentially creating closed captioning.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for closed captioning\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import transformers\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "import openai\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from langchain.document_loaders import AssemblyAIAudioTranscriptLoader\n",
    "from langchain.document_loaders import ImageCaptionLoader\n",
    "from langchain.document_loaders.assemblyai import TranscriptFormat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel:\n",
    "    def __init__(self, start_time, end_time, closed_caption):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.closed_caption = closed_caption\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"start_time: {self.start_time}, end_time: {self.end_time}, closed_caption: {self.closed_caption}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoModel:\n",
    "    def __init__(self, start_time, end_time, image_description):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.image_description = image_description\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"start_time: {self.start_time}, end_time: {self.end_time}, image_description: {self.image_description}\"\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self.start_time\n",
    "    \n",
    "    def get_end_time(self):\n",
    "        return self.end_time\n",
    "    \n",
    "    def get_image_description(self):\n",
    "        return self.image_description\n",
    "    \n",
    "    def set_start_time(self, start_time):\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def set_end_time(self, end_time):\n",
    "        self.end_time = end_time\n",
    "    \n",
    "    def set_image_description(self, image_description):\n",
    "        self.image_description = image_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioModel:\n",
    "    def __init__(self, start_time, end_time, subtitle_text):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.subtitle_text = subtitle_text\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"start_time: {self.start_time}, end_time: {self.end_time}, subtitle_text: {self.subtitle_text}\"\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self.start_time\n",
    "    \n",
    "    def get_end_time(self):\n",
    "        return self.end_time\n",
    "    \n",
    "    def get_subtitle_text(self):\n",
    "        return self.subtitle_text\n",
    "    \n",
    "    def set_start_time(self, start_time):\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def set_end_time(self, end_time):\n",
    "        self.end_time = end_time\n",
    "    \n",
    "    def set_subtitle_text(self, subtitle_text):\n",
    "        self.subtitle_text = subtitle_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = \"test_data/test.mp3\"\n",
    "\n",
    "loader = AssemblyAIAudioTranscriptLoader(\n",
    "    file_path=audio_file, \n",
    "    api_key=\"f50c08e20ecd4544b175953636f0b936\", \n",
    "    transcript_format=TranscriptFormat.SUBTITLES_SRT\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "def CreateTranscriptModel(doc):\n",
    "    transcription = doc.strip().split(\"\\n\")\n",
    "    times = transcription[1].split(\" --> \")\n",
    "    start_time = times[0].strip()\n",
    "    end_time = times[1].strip()\n",
    "\n",
    "    subtitle_text = ' '.join(transcription[2:]).strip()\n",
    "\n",
    "    transcript_model = AudioModel(start_time, end_time, subtitle_text)\n",
    "\n",
    "    return transcript_model\n",
    "\n",
    "print(CreateTranscriptModel(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Split Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mp3(mp4_path):\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(mp4_path)\n",
    "        .output(\"test_data/audio.mp3\", format=\"mp3\")\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "def CreateTranscriptModels(doc):\n",
    "    subtitles = doc.strip().split(\"\\n\\n\")  # Splitting based on double newline, which separates SRT entries\n",
    "    models = []\n",
    "\n",
    "    for subtitle in subtitles:\n",
    "        lines = subtitle.split(\"\\n\")\n",
    "        if len(lines) >= 3:  # Checking if there are enough lines for an index, timestamp, and text\n",
    "            times = lines[1].split(\" --> \")\n",
    "            start_time = times[0].strip()\n",
    "            end_time = times[1].strip()\n",
    "\n",
    "            subtitle_text = ' '.join(lines[2:]).strip()\n",
    "\n",
    "            transcript_model = AudioModel(start_time, end_time, subtitle_text)\n",
    "            models.append(transcript_model)\n",
    "\n",
    "    return models\n",
    "\n",
    "loader = AssemblyAIAudioTranscriptLoader(\n",
    "    file_path=\"test_data/audio.mp3\", \n",
    "    api_key=\"f50c08e20ecd4544b175953636f0b936\", \n",
    "    transcript_format=TranscriptFormat.SUBTITLES_SRT\n",
    ")\n",
    "\n",
    "# Assuming loader.load() returns the full transcript in a single document\n",
    "docs = loader.load()\n",
    "\n",
    "# This will now create a list of lists of AudioModel instances\n",
    "all_audio_models = [CreateTranscriptModels(doc.page_content) for doc in docs]\n",
    "\n",
    "# Flatten the list if necessary\n",
    "audio_models = [model for sublist in all_audio_models for model in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 00:00:00,570, end_time: 00:00:03,966, subtitle_text: Let's make tea. Duet with me. You read red,\n",
      "start_time: 00:00:04,068, end_time: 00:00:06,910, subtitle_text: I read green. Can I get you a drink?\n",
      "start_time: 00:00:09,250, end_time: 00:00:12,878, subtitle_text: Tea or coffee? How do\n",
      "start_time: 00:00:12,884, end_time: 00:00:13,920, subtitle_text: you take it?\n",
      "start_time: 00:00:16,050, end_time: 00:00:16,800, subtitle_text: Here.\n",
      "start_time: 00:00:19,810, end_time: 00:00:22,400, subtitle_text: Great work. Now let's try it again.\n"
     ]
    }
   ],
   "source": [
    "for audio_model in audio_models:\n",
    "    print(audio_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Split to Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_difference(prev_frame, curr_frame, threshold=30):\n",
    "    # Compute the absolute difference between the current frame and the previous frame\n",
    "    diff = cv2.absdiff(prev_frame, curr_frame)\n",
    "    # Thresholding to get the binary image, where white represents significant difference\n",
    "    _, thresh = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    # If there are any white pixels in thresh, the difference is significant\n",
    "    return np.any(thresh)\n",
    "\n",
    "# Initialize the video capture\n",
    "capture = cv2.VideoCapture('test_data/eng_convo.mp4')\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "frame_duration = 1000 / fps\n",
    "\n",
    "video_models = []\n",
    "\n",
    "frameNr = 0\n",
    "ret, prev_frame = capture.read()\n",
    "prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY) if ret else None\n",
    "prev_start_time = 0\n",
    "start_time = 0\n",
    "\n",
    "while ret:\n",
    "    end_time = prev_start_time\n",
    "    prev_start_time = capture.get(cv2.CAP_PROP_POS_MSEC)\n",
    "\n",
    "    ret, frame = capture.read()\n",
    "    if not ret:\n",
    "        start_time = end_time + frame_duration\n",
    "        break\n",
    "    \n",
    "    start_time = prev_start_time\n",
    "    # Convert to grayscale for comparison\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compare with the previous frame\n",
    "    if frameNr == 0 or frame_difference(prev_frame_gray, frame_gray):\n",
    "        end_time = capture.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        cv2.imwrite(f'test_data/output_frames/frame.jpg', frame)\n",
    "        prev_frame_gray = frame_gray\n",
    "\n",
    "        # Define the path to the \"output_frames\" folder\n",
    "        folder_path = f'test_data/output_frames/'\n",
    "\n",
    "        # List all .jpg files in the folder\n",
    "        image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".jpg\")]\n",
    "\n",
    "        # Create an instance of the ImageCaptionLoader\n",
    "        loader = ImageCaptionLoader(images=image_files)\n",
    "\n",
    "        # Load captions for the images\n",
    "        list_docs = loader.load()\n",
    "\n",
    "        video_model = VideoModel(start_time, end_time, list_docs[len(list_docs) - 1].page_content.replace(\"[SEP]\", \"\").strip())\n",
    "        video_models.append(video_model)\n",
    "\n",
    "        frameNr += 1\n",
    "\n",
    "# Release the video capture object\n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 0.0, end_time: 40.0, image_description: an image of a kitchen area in a building\n",
      "start_time: 40.0, end_time: 80.0, image_description: an image of a kitchen in a building\n"
     ]
    }
   ],
   "source": [
    "for video_model in video_models:\n",
    "    print(video_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the SRT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def milliseconds_to_srt_time(milliseconds):\n",
    "    seconds, milliseconds = divmod(milliseconds, 1000)\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{int(milliseconds):03}\"\n",
    "\n",
    "# Using video_models create an SRT file\n",
    "def CreateSRTFile(video_models):\n",
    "    file = open(\"test_data/output.srt\", \"w\")\n",
    "    count = 1\n",
    "    for video_model in video_models:\n",
    "        file.write(str(count) + \"\\n\")\n",
    "        file.write(str(milliseconds_to_srt_time(video_model.get_start_time())) + \" --> \" + str(milliseconds_to_srt_time(video_model.get_end_time())) + \"\\n\")\n",
    "        file.write(video_model.get_image_description() + \"\\n\\n\")\n",
    "        count += 1\n",
    "    file.close()\n",
    "\n",
    "CreateSRTFile(video_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio and Video Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 0.0, end_time: 40.0, image_description: an image of a kitchen area in a building\n",
      "start_time: 480.0, end_time: 2240.0, image_description: a young girl in various settings\n",
      "start_time: 2320.0, end_time: 9320.0, image_description: footage of a woman in a sweater appears repeatedly throughout this segment\n",
      "start_time: 12360.0, end_time: 14200.0, image_description: the same image of a woman in a sweater is shown at multiple intervals\n",
      "start_time: 15960.0, end_time: 22360.0, image_description: multiple instances of a woman holding a cup of coffee\n"
     ]
    }
   ],
   "source": [
    "# Assuming the VideoModel class definition is already in scope\n",
    "\n",
    "caption_data = \"\"\"\n",
    "start_time: 0.0, end_time: 40.0, image_description: an image of a kitchen area in a building\n",
    "start_time: 480.0, end_time: 2240.0, image_description: a young girl in various settings, last seen wearing a gray sweater\n",
    "start_time: 2320.0, end_time: 9320.0, image_description: footage of a woman in a sweater appears repeatedly throughout this segment\n",
    "start_time: 12360.0, end_time: 14200.0, image_description: the same image of a woman in a sweater is shown at multiple intervals\n",
    "start_time: 15960.0, end_time: 22360.0, image_description: multiple instances of a woman holding a cup of coffee\n",
    "\"\"\"\n",
    "\n",
    "# Parsing the raw caption data and instantiating VideoModel objects\n",
    "video_models = []\n",
    "for line in caption_data.strip().split(\"\\n\"):\n",
    "    if not line.strip():\n",
    "        continue  # Skip empty lines\n",
    "    parts = line.split(\",\")\n",
    "    start_time = float(parts[0].split(\":\")[1].strip())\n",
    "    end_time = float(parts[1].split(\":\")[1].strip())\n",
    "    image_description = parts[2].split(\":\")[1].strip()\n",
    "\n",
    "    video_model = VideoModel(start_time, end_time, image_description)\n",
    "    video_models.append(video_model)\n",
    "\n",
    "# Display the created VideoModel objects\n",
    "for vm in video_models:\n",
    "    print(vm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(s):\n",
    "    \"\"\"Converts a time string into seconds.\"\"\"\n",
    "    h, m, s = map(float, s.replace(',', '.').split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "def find_overlapping_models(video_models, audio_models):\n",
    "    primary_start = video_models.get_start_time()\n",
    "    primary_end = video_models.get_end_time()\n",
    "    overlapping_models = []\n",
    "    for model in audio_models:\n",
    "        model_start = parse_time(str(model.get_start_time()))\n",
    "        model_end = parse_time(str(model.get_end_time()))\n",
    "        if model_start < primary_end and model_end > primary_start:\n",
    "            overlapping_models.append(model)\n",
    "    return overlapping_models\n",
    "\n",
    "def validate_and_adjust_description(audio_model, video_model):\n",
    "    # Construct the prompt for OpenAI\n",
    "    prompt = f\"Does the following subtitle make sense: '{audio_model.get_subtitle_text()}' with the image described as '{video_model.get_image_description()}'?\"\n",
    "\n",
    "    # Define openai API key\n",
    "    openai.api_key = \"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\"\n",
    "\n",
    "    # Query OpenAI (ensure your API key and the model are correctly set)\n",
    "    response = openai.Completion.create(\n",
    "      engine=\"text-davinci-003\",\n",
    "      prompt=prompt,\n",
    "      max_tokens=50\n",
    "    )\n",
    "\n",
    "    # Interpret the response (this part might require tuning based on response structure)\n",
    "    # For simplicity, let's assume if response is not affirmative, we adjust the description\n",
    "    if 'yes' not in response.choices[0].text.lower():\n",
    "        # Here, you would implement logic to adjust the image description based on the audio description\n",
    "        # This could be a complex task depending on the context and might require additional natural language processing\n",
    "        new_description = \"adjusted description based on audio\"  # Placeholder\n",
    "        video_model.set_image_description(new_description)\n",
    "\n",
    "def create_caption_models(video_models, audio_models):\n",
    "    caption_models = []\n",
    "\n",
    "    for video_model in video_models:\n",
    "        overlapping_audio_models = find_overlapping_models(video_model, audio_models)\n",
    "        for audio_model in overlapping_audio_models:\n",
    "            validate_and_adjust_description(audio_model, video_model)\n",
    "            caption = f\"[{video_model.get_image_description()}] {audio_model.get_subtitle_text()}\"\n",
    "            caption_model = CaptionModel(video_model.get_start_time(), audio_model.get_end_time(), caption)\n",
    "            caption_models.append(caption_model)\n",
    "\n",
    "    return caption_models  \n",
    "\n",
    "# Create caption models\n",
    "caption_models = create_caption_models(video_models, audio_models)\n",
    "\n",
    "# Now, caption_models contains your finalized caption models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all caption models\n",
    "for caption_model in caption_models:\n",
    "    print(caption_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
