{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Captioning\n",
    "This notebook shows how to use \n",
    "Implements a feature using Langchain's image_captions.py and audio_speech_to_text.py to produce .srt files. This system will provide both subtitles and visual scene descriptions, essentially creating closed captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new VideoCaptioningChain chain...\u001b[0m\n",
      "Loading processors...\n",
      "Finished loading processors.\n",
      "Generating subtitles from audio...\n",
      "Finished generating subtitles:\n",
      "start_time: 5210, end_time: 9166, subtitle_text: The golden sand and gentle tropical breezes make the entire coast a\n",
      "start_time: 9188, end_time: 11450, subtitle_text: paradise for sun lovers of all ages.\n",
      "Generating closed captions from video...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\tomsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating closed captions:\n",
      "start_time: 0, end_time: 3233.3333333333335, image_description: an image of a man and woman sitting in chairs on the beach\n",
      "start_time: 3233.3333333333335, end_time: 4800.0, image_description: an image of a man sitting in a chair\n",
      "start_time: 4800.0, end_time: 5166.666666666667, image_description: an image of a man sitting in a chair on the beach\n",
      "start_time: 5166.666666666667, end_time: 5400.0, image_description: an image of two men sitting on a beach chair\n",
      "start_time: 5400.0, end_time: 5500.0, image_description: an image of a group of people sitting on a beach\n",
      "start_time: 5500.0, end_time: 5600.0, image_description: an image of a group of people sitting on a beach\n",
      "start_time: 5600.0, end_time: 5700.0, image_description: an image of a group of people sitting on a beach\n",
      "start_time: 5700.0, end_time: 5800.0, image_description: an image of a group of men sitting on a beach\n",
      "start_time: 5800.0, end_time: 5900.0, image_description: an image of two men sitting on a beach\n",
      "start_time: 5900.0, end_time: 6133.333333333333, image_description: an image of a group of people sitting on a beach\n",
      "start_time: 6133.333333333333, end_time: 8900.0, image_description: an image of a group of people sitting on chairs\n",
      "start_time: 8900.0, end_time: 9133.333333333332, image_description: an image of a group of people sitting on chairs\n",
      "start_time: 9133.333333333332, end_time: 9633.333333333332, image_description: an image of a man and woman sitting on chairs\n",
      "start_time: 9633.333333333332, end_time: 10000.0, image_description: an image of a man and woman sitting on chairs\n",
      "start_time: 10000.0, end_time: 10100.0, image_description: an image of a man and woman sitting on chairs\n",
      "start_time: 10100.0, end_time: 10200.0, image_description: an image of a man and woman sitting on chairs\n",
      "start_time: 10200.0, end_time: 10300.0, image_description: an image of a man and woman sitting on chairs\n",
      "start_time: 10300.0, end_time: 10533.333333333334, image_description: an image of a man and woman sitting on a beach chair\n",
      "start_time: 10533.333333333334, end_time: 10633.333333333332, image_description: an image of a man and woman sitting on chairs\n",
      "start_time: 10633.333333333332, end_time: 10866.666666666668, image_description: an image of a man and woman sitting on a beach chair\n",
      "start_time: 10866.666666666668, end_time: 10966.666666666666, image_description: an image of a man and woman sitting on chairs\n",
      "start_time: 10966.666666666666, end_time: 11333.333333333334, image_description: an image of a man and woman sitting in chairs\n",
      "start_time: 11333.333333333334, end_time: 11433.333333333334, image_description: an image of a man and woman sitting in chairs\n",
      "start_time: 11433.333333333334, end_time: 11533.333333333334, image_description: an image of a man and woman sitting in chairs\n",
      "start_time: 11533.333333333334, end_time: 12033.333333333334, image_description: an image of a woman in a bikini sitting on a beach chair\n",
      "start_time: 12033.333333333334, end_time: 12400.0, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 12400.0, end_time: 12500.0, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 12500.0, end_time: 12600.0, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 12600.0, end_time: 12700.0, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 12700.0, end_time: 12800.0, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 12800.0, end_time: 13166.666666666666, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 13166.666666666666, end_time: 13933.333333333334, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 13933.333333333334, end_time: 14033.333333333334, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 14033.333333333334, end_time: 14799.999999999998, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 14799.999999999998, end_time: 15033.333333333334, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 15033.333333333334, end_time: 15133.333333333332, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 15133.333333333332, end_time: 15233.333333333332, image_description: an image of a woman sitting in a chair on the beach\n",
      "start_time: 15233.333333333332, end_time: 15333.333333333334, image_description: an image of a woman in a bikini on the beach\n",
      "start_time: 15333.333333333334, end_time: 15433.333333333334, image_description: an image of a woman in a bikini sitting on a beach chair\n",
      "start_time: 15433.333333333334, end_time: 15533.333333333334, image_description: an image of a woman in a bikini on the beach\n",
      "start_time: 15533.333333333334, end_time: 15633.333333333332, image_description: an image of a woman in a bikini sitting on a beach chair\n",
      "Refining closed captions...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "I will provide you with several descriptions depicting events in one scene.\n",
      "Your task is to combine these descriptions into one description that contains only the important details from all descriptions.\n",
      "Especially if the two descriptions are very similar, make sure your response doesn't repeat itself.\n",
      "IMPORTANT: Do not make up a description. Do not make up events or anything that happened outside of the descriptions I am to provide you.\n",
      "I will now provide an example for you to learn from:\n",
      "Example: Description 1: The cat is at the beach, Description 2: The cat is eating lunch, Description 3: The cat is enjoying his time with friends\n",
      "Result: The cat is at the beach, eating lunch with his friends\n",
      "Now that I gave you the example, I will explain to you what exactly you need to return:\n",
      "Just give back one description, the description which is a combination of the descriptions you are provided with.\n",
      "Do not include anything else in your response other than the combined description.\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "Here is the data for you to work with in order to formulate your response:\n",
      "\n",
      "Human: Description 1: an image of a man and woman sitting in chairs on the beach, Description 2: an image of a man sitting in a chair, Description 3: an image of a man sitting in a chair on the beach, Description 4: an image of two men sitting on a beach chair, Description 5: an image of a group of people sitting on a beach, Description 6: an image of a group of people sitting on a beach, Description 7: an image of a group of people sitting on a beach, Description 8: an image of a group of men sitting on a beach, Description 9: an image of two men sitting on a beach, Description 10: an image of a group of people sitting on a beach, Description 11: an image of a group of people sitting on chairs, Description 12: an image of a group of people sitting on chairs, Description 13: an image of a man and woman sitting on chairs, Description 14: an image of a man and woman sitting on chairs, Description 15: an image of a man and woman sitting on chairs, Description 16: an image of a man and woman sitting on chairs, Description 17: an image of a man and woman sitting on chairs, Description 18: an image of a man and woman sitting on a beach chair, Description 19: an image of a man and woman sitting on chairs, Description 20: an image of a man and woman sitting on a beach chair, Description 21: an image of a man and woman sitting on chairs, Description 22: an image of a man and woman sitting in chairs, Description 23: an image of a man and woman sitting in chairs, Description 24: an image of a man and woman sitting in chairs, Description 25: an image of a woman sitting in a chair on the beach, Description 26: an image of a woman sitting in a chair on the beach, Description 27: an image of a woman sitting in a chair on the beach, Description 28: an image of a woman sitting in a chair on the beach, Description 29: an image of a woman sitting in a chair on the beach, Description 30: an image of a woman sitting in a chair on the beach, Description 31: an image of a woman sitting in a chair on the beach, Description 32: an image of a woman sitting in a chair on the beach, Description 33: an image of a woman sitting in a chair on the beach, Description 34: an image of a woman sitting in a chair on the beach, Description 35: an image of a woman sitting in a chair on the beach, Description 36: an image of a woman sitting in a chair on the beach\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Given a closed-caption description of an image or scene, remove any common prefixes like \"an image of,\" \"a scene of,\" or \"footage of.\" \n",
      "For instance, if the description is \"an image of a beautiful landscape,\" the modified version should be \"a beautiful landscape.\"\n",
      "\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: an image of a beautiful landscape\n",
      "Result: a beautiful landscape\n",
      "\n",
      "Input: a scene of people enjoying a picnic\n",
      "Result: people enjoying a picnic\n",
      "\n",
      "Below is the input for you to generate the result from:\n",
      "\n",
      "Human: Input: An image of a group of people including a man and a woman sitting in chairs on the beach.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "I will provide you with several descriptions depicting events in one scene.\n",
      "Your task is to combine these descriptions into one description that contains only the important details from all descriptions.\n",
      "Especially if the two descriptions are very similar, make sure your response doesn't repeat itself.\n",
      "IMPORTANT: Do not make up a description. Do not make up events or anything that happened outside of the descriptions I am to provide you.\n",
      "I will now provide an example for you to learn from:\n",
      "Example: Description 1: The cat is at the beach, Description 2: The cat is eating lunch, Description 3: The cat is enjoying his time with friends\n",
      "Result: The cat is at the beach, eating lunch with his friends\n",
      "Now that I gave you the example, I will explain to you what exactly you need to return:\n",
      "Just give back one description, the description which is a combination of the descriptions you are provided with.\n",
      "Do not include anything else in your response other than the combined description.\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "Here is the data for you to work with in order to formulate your response:\n",
      "\n",
      "Human: Description 1: an image of a woman in a bikini on the beach, Description 2: an image of a woman in a bikini on the beach\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Given a closed-caption description of an image or scene, remove any common prefixes like \"an image of,\" \"a scene of,\" or \"footage of.\" \n",
      "For instance, if the description is \"an image of a beautiful landscape,\" the modified version should be \"a beautiful landscape.\"\n",
      "\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: an image of a beautiful landscape\n",
      "Result: a beautiful landscape\n",
      "\n",
      "Input: a scene of people enjoying a picnic\n",
      "Result: people enjoying a picnic\n",
      "\n",
      "Below is the input for you to generate the result from:\n",
      "\n",
      "Human: Input: An image of a woman in a bikini on the beach.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Finished refining closed captions:\n",
      "start_time: 0, end_time: 15233.333333333332, image_description: a group of people including a man and a woman sitting in chairs on the beach.\n",
      "start_time: 15233.333333333332, end_time: 15633.333333333332, image_description: a woman in a bikini on the beach.\n",
      "Combining subtitles with closed captions...\n",
      "0, 15233.333333333332\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are tasked with enhancing closed-caption descriptions based on corresponding subtitles from the audio of a real movie clip. \n",
      "Assignment details, from highest to lowest priority:\n",
      "\n",
      "1) If the subtitle exceeds {limit} characters, creatively rewrite the description to not exceed the character limit, preserving as many details as you can.\n",
      "    If you feel that you cannot complete the response under the character limit, you must omit details in order to remain below the character limit.\n",
      "    \n",
      "2) If the details in the subtitle provide meaningful additional information to its closed-caption description, incorporate those details into the description.\n",
      "\n",
      "Enhance the closed-caption description by integrating details from the subtitle if they contribute meaningful information.\n",
      "\n",
      "Example:\n",
      "Subtitle: car screeching, tires squealing\n",
      "Closed-Caption Description: A car speeds down the street.\n",
      "\n",
      "Output: Result: A car speeds down the street, its tires screeching and squealing.\n",
      "\n",
      "**IMPORTANT**: Remember your assignment details when formulating your response! YOU MUST NOT EXCEED {limit} CHARACTERS.\n",
      "\n",
      "***IMPORTANT***: You must only return the following text in your response. You may not return a response that does not follow the exact format in the next line:\n",
      "Result: Text\n",
      "\n",
      "**** YOU MUST PROVIDE ME WITH THE BEST ANSWER YOU CAN COME UP WITH,\n",
      "**** EVEN IF YOU DEEM THAT IT IS A BAD ONE. YOU MUST ONLY RESPOND IN THE FORMAT IN THE NEXT LINE:\n",
      "Result: Text\n",
      "\n",
      "Below is the data provided, generate a response using this data:\n",
      "\n",
      "Human: Subtitle: The golden sand and gentle tropical breezes make the entire coast a\n",
      "Closed-Caption Description: a group of people including a man and a woman sitting in chairs on the beach.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are tasked with enhancing closed-caption descriptions based on corresponding subtitles from the audio of a real movie clip. \n",
      "Assignment details, from highest to lowest priority:\n",
      "\n",
      "1) If the subtitle exceeds {limit} characters, creatively rewrite the description to not exceed the character limit, preserving as many details as you can.\n",
      "    If you feel that you cannot complete the response under the character limit, you must omit details in order to remain below the character limit.\n",
      "    \n",
      "2) If the details in the subtitle provide meaningful additional information to its closed-caption description, incorporate those details into the description.\n",
      "\n",
      "Enhance the closed-caption description by integrating details from the subtitle if they contribute meaningful information.\n",
      "\n",
      "Example:\n",
      "Subtitle: car screeching, tires squealing\n",
      "Closed-Caption Description: A car speeds down the street.\n",
      "\n",
      "Output: Result: A car speeds down the street, its tires screeching and squealing.\n",
      "\n",
      "**IMPORTANT**: Remember your assignment details when formulating your response! YOU MUST NOT EXCEED {limit} CHARACTERS.\n",
      "\n",
      "***IMPORTANT***: You must only return the following text in your response. You may not return a response that does not follow the exact format in the next line:\n",
      "Result: Text\n",
      "\n",
      "**** YOU MUST PROVIDE ME WITH THE BEST ANSWER YOU CAN COME UP WITH,\n",
      "**** EVEN IF YOU DEEM THAT IT IS A BAD ONE. YOU MUST ONLY RESPOND IN THE FORMAT IN THE NEXT LINE:\n",
      "Result: Text\n",
      "\n",
      "Below is the data provided, generate a response using this data:\n",
      "\n",
      "Human: Subtitle: paradise for sun lovers of all ages.\n",
      "Closed-Caption Description: a group of people including a man and a woman sitting in chairs on the beach.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "15233.333333333332, 15633.333333333332\n",
      "Finished combining subtitles with closed captions:\n",
      "start_time: 5210, end_time: 9166, closed_caption: [A man and a woman, part of a group, sit on beach chairs, relishing the golden sand and gentle tropical breezes along the coast.] The golden sand and gentle tropical breezes make the entire coast a\n",
      "start_time: 9188, end_time: 11450, closed_caption: [A man and woman, among others, lounge on the beach, a paradise for sun lovers of all ages.] paradise for sun lovers of all ages.\n",
      "start_time: 11450, end_time: 15233.333333333332, closed_caption: [a group of people including a man and a woman sitting in chairs on the beach.]\n",
      "start_time: 15233.333333333332, end_time: 15633.333333333332, closed_caption: [a woman in a bikini on the beach.]\n",
      "Generating SRT file...\n",
      "Finished generating srt file.\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "00:00:05,210 --> 00:00:09,166\n",
      "[A man and a woman, part of a group, sit on beach chairs, relishing the golden sand and gentle tropical breezes along the coast.] The golden sand and gentle tropical breezes make the entire coast a\n",
      "\n",
      "2\n",
      "00:00:09,188 --> 00:00:11,450\n",
      "[A man and woman, among others, lounge on the beach, a paradise for sun lovers of all ages.] paradise for sun lovers of all ages.\n",
      "\n",
      "3\n",
      "00:00:11,450 --> 00:00:15,233\n",
      "[a group of people including a man and a woman sitting in chairs on the beach.]\n",
      "\n",
      "4\n",
      "00:00:15,233 --> 00:00:15,633\n",
      "[a woman in a bikini on the beach.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.stdout import StdOutCallbackHandler\n",
    "from langchain.chains.video_captioning import VideoCaptioningChain\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "#TODO: env variable for keys\n",
    "chain = VideoCaptioningChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-4\", max_tokens=4000, openai_api_key=\"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\"),\n",
    "    assemblyai_key=\"f50c08e20ecd4544b175953636f0b936\"\n",
    ")\n",
    "\n",
    "result = chain.run(video_file_path=\"test_data/test1.mp4\")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
