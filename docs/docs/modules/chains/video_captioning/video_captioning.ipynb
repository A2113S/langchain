{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Captioning\n",
    "This notebook shows how to use VideoCaptioningChain, which is implemented using Langchain's ImageCaptionLoader and AssemblyAI to produce .srt files.\n",
    "\n",
    "This system autogenerates both subtitles and closed captions from a video URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.video_captioning import VideoCaptioningChain\n",
    "from langchain.chat_models.openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "#ASSEMBLYAI_API_KEY = getpass.getpass(\"AssemblyAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Delete this cell before PR and uncomment above\n",
    "\n",
    "OPENAI_API_KEY = \"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\"\n",
    "ASSEMBLYAI_API_KEY = \"f50c08e20ecd4544b175953636f0b936\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required parameters:**\n",
    "\n",
    "* llm: The language model this chain will use to get suggestions on how to refine the closed-captions\n",
    "* assemblyai_key: The API key for AssemblyAI, used to generate the subtitles\n",
    "\n",
    "**Optional Parameters:**\n",
    "\n",
    "* verbose (Default: True): Sets verbose mode for downstream chain calls\n",
    "* use_logging (Default: True): Log the chain's processes in run manager\n",
    "* frame_skip (Default: None): Choose how many video frames to skip during processing. Increasing it results in faster execution, but less accurate results. If None, frame skip is calculated manually based on the framerate Set this to 0 to sample all frames\n",
    "* image_delta_threshold (Default: 3000000): Set the sensitivity for what the image processor considers a change in scenery in the video, used to delimit closed captions. Higher = less sensitive\n",
    "* closed_caption_char_limit (Default: 20): Sets the character limit on closed captions\n",
    "* closed_caption_similarity_threshold (Default: 80): Sets the percentage value to how similar two closed caption models should be in order to be clustered into one longer closed caption\n",
    "* use_unclustered_video_models (Default: False): If true, closed captions that could not be clustered will be included. May result in spontaneous behaviour from closed captions such as very short lasting captions or fast-changing captions. Enabling this is experimental and not recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new VideoCaptioningChain chain...\u001b[0m\n",
      "Loading processors...\n",
      "Finished loading processors.\n",
      "Generating subtitles from audio...\n",
      "Finished generating subtitles:\n",
      "start_time: 250, end_time: 4106, subtitle_text: Any chives? What were you eating?\n",
      "start_time: 4218, end_time: 8240, subtitle_text: Chives. We are not going off on some wild goose chase. Who's in?\n",
      "start_time: 9010, end_time: 12078, subtitle_text: How easy? A little tighter. In at the couch. Yeah, it's a little tight in\n",
      "start_time: 12084, end_time: 14620, subtitle_text: the couch. Did you eat?\n",
      "Generating closed captions from video...\n",
      "15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\tomsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tomsa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating closed captions:\n",
      "start_time: 0, end_time: 1083.3333333333333, image_description: an image of a man with a mustache and mustache\n",
      "start_time: 1083.3333333333333, end_time: 1333.3333333333333, image_description: an image of a man with a mustache and mustache\n",
      "start_time: 1333.3333333333333, end_time: 1583.3333333333333, image_description: an image of a man with a mustache and mustache\n",
      "start_time: 1583.3333333333333, end_time: 1833.3333333333333, image_description: an image of a man with a mustache and mustache\n",
      "start_time: 1833.3333333333333, end_time: 2083.3333333333335, image_description: an image of a man with a mustache and a mustache\n",
      "start_time: 2083.3333333333335, end_time: 2333.3333333333335, image_description: an image of a man with a mustache in an office\n",
      "start_time: 2333.3333333333335, end_time: 2583.3333333333335, image_description: an image of two people standing in front of a house\n",
      "start_time: 2583.3333333333335, end_time: 2833.3333333333335, image_description: an image of two people standing in front of a building\n",
      "start_time: 2833.3333333333335, end_time: 4150.0, image_description: an image of a woman and man standing in front of a house\n",
      "start_time: 4150.0, end_time: 4400.0, image_description: an image of two people standing in front of a building\n",
      "start_time: 4400.0, end_time: 4650.0, image_description: an image of the moon with the words moon\n",
      "start_time: 4650.0, end_time: 4900.0, image_description: an image of the moon with the words moon\n",
      "start_time: 4900.0, end_time: 5150.0, image_description: an image of the words no 4 on a black background\n",
      "start_time: 5150.0, end_time: 5400.0, image_description: an image of the words no 4 a in white on a black background\n",
      "start_time: 5400.0, end_time: 5650.0, image_description: an image of a group of people in a room\n",
      "start_time: 5650.0, end_time: 6433.333333333334, image_description: an image of a group of people in a room\n",
      "start_time: 6433.333333333334, end_time: 6683.333333333334, image_description: an image of a group of people in a room\n",
      "start_time: 6683.333333333334, end_time: 7466.666666666667, image_description: an image of a group of people in a room\n",
      "start_time: 7466.666666666667, end_time: 7716.666666666667, image_description: an image of a group of people in a room\n",
      "start_time: 7716.666666666667, end_time: 7966.666666666667, image_description: an image of a group of people in a room\n",
      "start_time: 7966.666666666667, end_time: 8216.666666666666, image_description: an image of a group of people in a room\n",
      "start_time: 8216.666666666666, end_time: 8466.666666666666, image_description: an image of a group of people in a room\n",
      "start_time: 8466.666666666666, end_time: 8716.666666666666, image_description: an image of the word every in green on a black background\n",
      "start_time: 8716.666666666666, end_time: 8966.666666666666, image_description: an image of the words every day in green on a black background\n",
      "start_time: 8966.666666666666, end_time: 9483.333333333332, image_description: an image of a man in a suit and a penguin\n",
      "start_time: 9483.333333333332, end_time: 9733.333333333332, image_description: an image of a man carrying a giant bag\n",
      "start_time: 9733.333333333332, end_time: 9983.333333333332, image_description: an image of a man carrying a giant bag\n",
      "start_time: 9983.333333333332, end_time: 10233.333333333332, image_description: an image of a man in a suit and tie standing on a street\n",
      "start_time: 10233.333333333332, end_time: 10483.333333333332, image_description: an image of a man in a suit and tie standing on a street\n",
      "start_time: 10483.333333333332, end_time: 10733.333333333332, image_description: an image of a man in a suit shop\n",
      "start_time: 10733.333333333332, end_time: 11250.0, image_description: an image of a man in a suit shop\n",
      "start_time: 11250.0, end_time: 12566.666666666666, image_description: an image of the simpsons family's burgers\n",
      "start_time: 12566.666666666666, end_time: 12816.666666666666, image_description: an image of the simpsons family's logo\n",
      "start_time: 12816.666666666666, end_time: 13066.666666666666, image_description: an image of the simpsons family's burgers\n",
      "start_time: 13066.666666666666, end_time: 13316.666666666666, image_description: an image of a quote that says the simpsons family do burgers\n",
      "start_time: 13316.666666666666, end_time: 14100.0, image_description: an image of a black and white poster with the words the simpsons family boys, boys, boys\n",
      "start_time: 14100.0, end_time: 14350.0, image_description: an image of a man in a suit walking through the woods\n",
      "start_time: 14350.0, end_time: 14600.0, image_description: an image of a robot in the woods\n",
      "start_time: 14600.0, end_time: 14850.0, image_description: an image of the letter x\n",
      "Refining closed captions...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "I will provide you with several descriptions depicting events in one scene.\n",
      "Your task is to combine these descriptions into one description that contains only the important details from all descriptions.\n",
      "Especially if the two descriptions are very similar, make sure your response doesn't repeat itself.\n",
      "IMPORTANT: Do not make up a description. Do not make up events or anything that happened outside of the descriptions I am to provide you.\n",
      "I will now provide an example for you to learn from:\n",
      "Example: Description 1: The cat is at the beach, Description 2: The cat is eating lunch, Description 3: The cat is enjoying his time with friends\n",
      "Result: The cat is at the beach, eating lunch with his friends\n",
      "Now that I gave you the example, I will explain to you what exactly you need to return:\n",
      "Just give back one description, the description which is a combination of the descriptions you are provided with.\n",
      "Do not include anything else in your response other than the combined description.\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "Here is the data for you to work with in order to formulate your response:\n",
      "\n",
      "Human: Description 1: an image of a man with a mustache and mustache, Description 2: an image of a man with a mustache and a mustache\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Given a closed-caption description of an image or scene, remove any common prefixes like \"an image of,\" \"a scene of,\" or \"footage of.\" \n",
      "For instance, if the description is \"an image of a beautiful landscape,\" the modified version should be \"a beautiful landscape.\"\n",
      "\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: an image of a beautiful landscape\n",
      "Result: a beautiful landscape\n",
      "\n",
      "Input: a scene of people enjoying a picnic\n",
      "Result: people enjoying a picnic\n",
      "\n",
      "Below is the input for you to generate the result from:\n",
      "\n",
      "Human: Input: An image of a man with a mustache.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "I will provide you with several descriptions depicting events in one scene.\n",
      "Your task is to combine these descriptions into one description that contains only the important details from all descriptions.\n",
      "Especially if the two descriptions are very similar, make sure your response doesn't repeat itself.\n",
      "IMPORTANT: Do not make up a description. Do not make up events or anything that happened outside of the descriptions I am to provide you.\n",
      "I will now provide an example for you to learn from:\n",
      "Example: Description 1: The cat is at the beach, Description 2: The cat is eating lunch, Description 3: The cat is enjoying his time with friends\n",
      "Result: The cat is at the beach, eating lunch with his friends\n",
      "Now that I gave you the example, I will explain to you what exactly you need to return:\n",
      "Just give back one description, the description which is a combination of the descriptions you are provided with.\n",
      "Do not include anything else in your response other than the combined description.\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "Here is the data for you to work with in order to formulate your response:\n",
      "\n",
      "Human: Description 1: an image of two people standing in front of a house, Description 2: an image of two people standing in front of a building, Description 3: an image of a woman and man standing in front of a house, Description 4: an image of two people standing in front of a building\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Given a closed-caption description of an image or scene, remove any common prefixes like \"an image of,\" \"a scene of,\" or \"footage of.\" \n",
      "For instance, if the description is \"an image of a beautiful landscape,\" the modified version should be \"a beautiful landscape.\"\n",
      "\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: an image of a beautiful landscape\n",
      "Result: a beautiful landscape\n",
      "\n",
      "Input: a scene of people enjoying a picnic\n",
      "Result: people enjoying a picnic\n",
      "\n",
      "Below is the input for you to generate the result from:\n",
      "\n",
      "Human: Input: An image of a woman and a man standing in front of a house.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "I will provide you with several descriptions depicting events in one scene.\n",
      "Your task is to combine these descriptions into one description that contains only the important details from all descriptions.\n",
      "Especially if the two descriptions are very similar, make sure your response doesn't repeat itself.\n",
      "IMPORTANT: Do not make up a description. Do not make up events or anything that happened outside of the descriptions I am to provide you.\n",
      "I will now provide an example for you to learn from:\n",
      "Example: Description 1: The cat is at the beach, Description 2: The cat is eating lunch, Description 3: The cat is enjoying his time with friends\n",
      "Result: The cat is at the beach, eating lunch with his friends\n",
      "Now that I gave you the example, I will explain to you what exactly you need to return:\n",
      "Just give back one description, the description which is a combination of the descriptions you are provided with.\n",
      "Do not include anything else in your response other than the combined description.\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "Here is the data for you to work with in order to formulate your response:\n",
      "\n",
      "Human: Description 1: an image of the words no 4 on a black background, Description 2: an image of the words no 4 a in white on a black background\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Given a closed-caption description of an image or scene, remove any common prefixes like \"an image of,\" \"a scene of,\" or \"footage of.\" \n",
      "For instance, if the description is \"an image of a beautiful landscape,\" the modified version should be \"a beautiful landscape.\"\n",
      "\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: an image of a beautiful landscape\n",
      "Result: a beautiful landscape\n",
      "\n",
      "Input: a scene of people enjoying a picnic\n",
      "Result: people enjoying a picnic\n",
      "\n",
      "Below is the input for you to generate the result from:\n",
      "\n",
      "Human: Input: An image of the white words \"no 4\" on a black background.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "I will provide you with several descriptions depicting events in one scene.\n",
      "Your task is to combine these descriptions into one description that contains only the important details from all descriptions.\n",
      "Especially if the two descriptions are very similar, make sure your response doesn't repeat itself.\n",
      "IMPORTANT: Do not make up a description. Do not make up events or anything that happened outside of the descriptions I am to provide you.\n",
      "I will now provide an example for you to learn from:\n",
      "Example: Description 1: The cat is at the beach, Description 2: The cat is eating lunch, Description 3: The cat is enjoying his time with friends\n",
      "Result: The cat is at the beach, eating lunch with his friends\n",
      "Now that I gave you the example, I will explain to you what exactly you need to return:\n",
      "Just give back one description, the description which is a combination of the descriptions you are provided with.\n",
      "Do not include anything else in your response other than the combined description.\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "Here is the data for you to work with in order to formulate your response:\n",
      "\n",
      "Human: Description 1: an image of the word every in green on a black background, Description 2: an image of the words every day in green on a black background\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Given a closed-caption description of an image or scene, remove any common prefixes like \"an image of,\" \"a scene of,\" or \"footage of.\" \n",
      "For instance, if the description is \"an image of a beautiful landscape,\" the modified version should be \"a beautiful landscape.\"\n",
      "\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: an image of a beautiful landscape\n",
      "Result: a beautiful landscape\n",
      "\n",
      "Input: a scene of people enjoying a picnic\n",
      "Result: people enjoying a picnic\n",
      "\n",
      "Below is the input for you to generate the result from:\n",
      "\n",
      "Human: Input: An image of the words 'every day' in green on a black background.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "I will provide you with several descriptions depicting events in one scene.\n",
      "Your task is to combine these descriptions into one description that contains only the important details from all descriptions.\n",
      "Especially if the two descriptions are very similar, make sure your response doesn't repeat itself.\n",
      "IMPORTANT: Do not make up a description. Do not make up events or anything that happened outside of the descriptions I am to provide you.\n",
      "I will now provide an example for you to learn from:\n",
      "Example: Description 1: The cat is at the beach, Description 2: The cat is eating lunch, Description 3: The cat is enjoying his time with friends\n",
      "Result: The cat is at the beach, eating lunch with his friends\n",
      "Now that I gave you the example, I will explain to you what exactly you need to return:\n",
      "Just give back one description, the description which is a combination of the descriptions you are provided with.\n",
      "Do not include anything else in your response other than the combined description.\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "Here is the data for you to work with in order to formulate your response:\n",
      "\n",
      "Human: Description 1: an image of the simpsons family's burgers, Description 2: an image of the simpsons family's logo, Description 3: an image of the simpsons family's burgers\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "Given a closed-caption description of an image or scene, remove any common prefixes like \"an image of,\" \"a scene of,\" or \"footage of.\" \n",
      "For instance, if the description is \"an image of a beautiful landscape,\" the modified version should be \"a beautiful landscape.\"\n",
      "\n",
      "IMPORTANT: the output in your response should be 'Result:text', where text is the description you generated.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Input: an image of a beautiful landscape\n",
      "Result: a beautiful landscape\n",
      "\n",
      "Input: a scene of people enjoying a picnic\n",
      "Result: people enjoying a picnic\n",
      "\n",
      "Below is the input for you to generate the result from:\n",
      "\n",
      "Human: Input: An image of the Simpsons family's burgers, alongside their logo.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Finished refining closed captions:\n",
      "start_time: 0, end_time: 2083.3333333333335, image_description: a man with a mustache.\n",
      "start_time: 2333.3333333333335, end_time: 4400.0, image_description: a woman and a man standing in front of a house.\n",
      "start_time: 4900.0, end_time: 5400.0, image_description: the white words \"no 4\" on a black background.\n",
      "start_time: 8466.666666666666, end_time: 8966.666666666666, image_description: the words 'every day' in green on a black background.\n",
      "start_time: 11250.0, end_time: 13066.666666666666, image_description: the Simpsons family's burgers, alongside their logo.\n",
      "Combining subtitles with closed captions...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are tasked with enhancing closed-caption descriptions based on corresponding subtitles from the audio of a real movie clip. \n",
      "Assignment details, from highest to lowest priority:\n",
      "\n",
      "1) If the subtitle exceeds {limit} characters, creatively rewrite the description to not exceed the character limit, preserving as many details as you can.\n",
      "    If you feel that you cannot complete the response under the character limit, you must omit details in order to remain below the character limit.\n",
      "    \n",
      "2) If the details in the subtitle provide meaningful additional information to its closed-caption description, incorporate those details into the description.\n",
      "\n",
      "Enhance the closed-caption description by integrating details from the subtitle if they contribute meaningful information.\n",
      "\n",
      "Example:\n",
      "Subtitle: car screeching, tires squealing\n",
      "Closed-Caption Description: A car speeds down the street.\n",
      "\n",
      "Output: Result: A car speeds down the street, its tires screeching and squealing.\n",
      "\n",
      "**IMPORTANT**: Remember your assignment details when formulating your response! YOU MUST NOT EXCEED {limit} CHARACTERS.\n",
      "\n",
      "***IMPORTANT***: You must only return the following text in your response. You may not return a response that does not follow the exact format in the next line:\n",
      "Result: Text\n",
      "\n",
      "**** YOU MUST PROVIDE ME WITH THE BEST ANSWER YOU CAN COME UP WITH,\n",
      "**** EVEN IF YOU DEEM THAT IT IS A BAD ONE. YOU MUST ONLY RESPOND IN THE FORMAT IN THE NEXT LINE:\n",
      "Result: Text\n",
      "\n",
      "Below is the data provided, generate a response using this data:\n",
      "\n",
      "Human: Subtitle: Any chives? What were you eating?\n",
      "Closed-Caption Description: a man with a mustache.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are tasked with enhancing closed-caption descriptions based on corresponding subtitles from the audio of a real movie clip. \n",
      "Assignment details, from highest to lowest priority:\n",
      "\n",
      "1) If the subtitle exceeds {limit} characters, creatively rewrite the description to not exceed the character limit, preserving as many details as you can.\n",
      "    If you feel that you cannot complete the response under the character limit, you must omit details in order to remain below the character limit.\n",
      "    \n",
      "2) If the details in the subtitle provide meaningful additional information to its closed-caption description, incorporate those details into the description.\n",
      "\n",
      "Enhance the closed-caption description by integrating details from the subtitle if they contribute meaningful information.\n",
      "\n",
      "Example:\n",
      "Subtitle: car screeching, tires squealing\n",
      "Closed-Caption Description: A car speeds down the street.\n",
      "\n",
      "Output: Result: A car speeds down the street, its tires screeching and squealing.\n",
      "\n",
      "**IMPORTANT**: Remember your assignment details when formulating your response! YOU MUST NOT EXCEED {limit} CHARACTERS.\n",
      "\n",
      "***IMPORTANT***: You must only return the following text in your response. You may not return a response that does not follow the exact format in the next line:\n",
      "Result: Text\n",
      "\n",
      "**** YOU MUST PROVIDE ME WITH THE BEST ANSWER YOU CAN COME UP WITH,\n",
      "**** EVEN IF YOU DEEM THAT IT IS A BAD ONE. YOU MUST ONLY RESPOND IN THE FORMAT IN THE NEXT LINE:\n",
      "Result: Text\n",
      "\n",
      "Below is the data provided, generate a response using this data:\n",
      "\n",
      "Human: Subtitle: Chives. We are not going off on some wild goose chase. Who's in?\n",
      "Closed-Caption Description: a woman and a man standing in front of a house.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are tasked with enhancing closed-caption descriptions based on corresponding subtitles from the audio of a real movie clip. \n",
      "Assignment details, from highest to lowest priority:\n",
      "\n",
      "1) If the subtitle exceeds {limit} characters, creatively rewrite the description to not exceed the character limit, preserving as many details as you can.\n",
      "    If you feel that you cannot complete the response under the character limit, you must omit details in order to remain below the character limit.\n",
      "    \n",
      "2) If the details in the subtitle provide meaningful additional information to its closed-caption description, incorporate those details into the description.\n",
      "\n",
      "Enhance the closed-caption description by integrating details from the subtitle if they contribute meaningful information.\n",
      "\n",
      "Example:\n",
      "Subtitle: car screeching, tires squealing\n",
      "Closed-Caption Description: A car speeds down the street.\n",
      "\n",
      "Output: Result: A car speeds down the street, its tires screeching and squealing.\n",
      "\n",
      "**IMPORTANT**: Remember your assignment details when formulating your response! YOU MUST NOT EXCEED {limit} CHARACTERS.\n",
      "\n",
      "***IMPORTANT***: You must only return the following text in your response. You may not return a response that does not follow the exact format in the next line:\n",
      "Result: Text\n",
      "\n",
      "**** YOU MUST PROVIDE ME WITH THE BEST ANSWER YOU CAN COME UP WITH,\n",
      "**** EVEN IF YOU DEEM THAT IT IS A BAD ONE. YOU MUST ONLY RESPOND IN THE FORMAT IN THE NEXT LINE:\n",
      "Result: Text\n",
      "\n",
      "Below is the data provided, generate a response using this data:\n",
      "\n",
      "Human: Subtitle: How easy? A little tighter. In at the couch. Yeah, it's a little tight in\n",
      "Closed-Caption Description: the Simpsons family's burgers, alongside their logo.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Finished combining subtitles with closed captions:\n",
      "start_time: 0, end_time: 250, closed_caption: [a man with a mustache.]\n",
      "start_time: 250, end_time: 2083.3333333333335, closed_caption: [A mustached man questions, \"Any chives? What were you eating?\"] Any chives? What were you eating?\n",
      "start_time: 2083.3333333333335, end_time: 4106, closed_caption: Any chives? What were you eating?\n",
      "start_time: 2333.3333333333335, end_time: 4218, closed_caption: [a woman and a man standing in front of a house.]\n",
      "start_time: 4218, end_time: 4400.0, closed_caption: [A woman and a man stand before a house, discussing. \"Chives. We're not going off on some wild goose chase. Who's in?\" she asks.] Chives. We are not going off on some wild goose chase. Who's in?\n",
      "start_time: 4400.0, end_time: 8240, closed_caption: Chives. We are not going off on some wild goose chase. Who's in?\n",
      "start_time: 4900.0, end_time: 5400.0, closed_caption: [the white words \"no 4\" on a black background.]\n",
      "start_time: 8466.666666666666, end_time: 8966.666666666666, closed_caption: [the words 'every day' in green on a black background.]\n",
      "start_time: 9010, end_time: 11250.0, closed_caption: How easy? A little tighter. In at the couch. Yeah, it's a little tight in\n",
      "start_time: 11250.0, end_time: 12078, closed_caption: [The Simpsons family's burgers are presented, their logo visible. A voice questions, \"How easy?\" and comments on the tight space at the couch.] How easy? A little tighter. In at the couch. Yeah, it's a little tight in\n",
      "start_time: 12078, end_time: 13066.666666666666, closed_caption: [the Simpsons family's burgers, alongside their logo.]\n",
      "Generating SRT file...\n",
      "Finished generating srt file.\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1\n",
      "00:00:00,000 --> 00:00:00,250\n",
      "[a man with a mustache.]\n",
      "\n",
      "2\n",
      "00:00:00,250 --> 00:00:02,083\n",
      "[A mustached man questions, \"Any chives? What were you eating?\"] Any chives? What were you eating?\n",
      "\n",
      "3\n",
      "00:00:02,083 --> 00:00:04,106\n",
      "Any chives? What were you eating?\n",
      "\n",
      "4\n",
      "00:00:02,333 --> 00:00:04,218\n",
      "[a woman and a man standing in front of a house.]\n",
      "\n",
      "5\n",
      "00:00:04,218 --> 00:00:04,400\n",
      "[A woman and a man stand before a house, discussing. \"Chives. We're not going off on some wild goose chase. Who's in?\" she asks.] Chives. We are not going off on some wild goose chase. Who's in?\n",
      "\n",
      "6\n",
      "00:00:04,400 --> 00:00:08,240\n",
      "Chives. We are not going off on some wild goose chase. Who's in?\n",
      "\n",
      "7\n",
      "00:00:04,900 --> 00:00:05,400\n",
      "[the white words \"no 4\" on a black background.]\n",
      "\n",
      "8\n",
      "00:00:08,466 --> 00:00:08,966\n",
      "[the words 'every day' in green on a black background.]\n",
      "\n",
      "9\n",
      "00:00:09,010 --> 00:00:11,250\n",
      "How easy? A little tighter. In at the couch. Yeah, it's a little tight in\n",
      "\n",
      "10\n",
      "00:00:11,250 --> 00:00:12,078\n",
      "[The Simpsons family's burgers are presented, their logo visible. A voice questions, \"How easy?\" and comments on the tight space at the couch.] How easy? A little tighter. In at the couch. Yeah, it's a little tight in\n",
      "\n",
      "11\n",
      "00:00:12,078 --> 00:00:13,066\n",
      "[the Simpsons family's burgers, alongside their logo.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://ia804703.us.archive.org/27/items/uh-oh-here-we-go-again/Uh-Oh%2C%20Here%20we%20go%20again.mp4\n",
    "# https://ia601200.us.archive.org/9/items/f58703d4-61e6-4f8f-8c08-b42c7e16f7cb/f58703d4-61e6-4f8f-8c08-b42c7e16f7cb.mp4\n",
    "\n",
    "chain = VideoCaptioningChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-4\",\n",
    "    max_tokens=4000,\n",
    "    openai_api_key=OPENAI_API_KEY),\n",
    "    assemblyai_key=ASSEMBLYAI_API_KEY,\n",
    ")\n",
    "\n",
    "srt_content = chain.run(\n",
    "    video_file_path=\n",
    "    \"https://ia904700.us.archive.org/22/items/any-chibes/X2Download.com-FXX%20USA%20%C2%ABPromo%20Noon%20-%204A%20Every%20Day%EF%BF%BD%EF%BF%BD%C2%BB%20November%202021%EF%BF%BD%EF%BF%BD-%281080p60%29.mp4\"\n",
    ")\n",
    "\n",
    "print(srt_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing output to .srt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.srt', 'w') as file:\n",
    "    file.write(srt_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
